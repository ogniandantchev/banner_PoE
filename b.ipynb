{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection with Vision Transformers\n",
    "Extracting web banners from screenshots\n",
    "\n",
    "\n",
    "**Author:** Ognian Dantchev, based on the Keras team code [example](https://keras.io/examples/vision/object_detection_using_vision_transformer/) by [Karan V. Dave](https://www.linkedin.com/in/karan-dave-811413164/).\n",
    "\n",
    "**Created:** 2023/02/04\n",
    "\n",
    "**Description:** A simple Keras implementation of object detection using Vision Transformers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The article\n",
    "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929),\n",
    "architecture by Alexey Dosovitskiy et al.\n",
    "demonstrates that a pure transformer applied directly to sequences of image\n",
    "patches can perform well on object detection tasks.\n",
    "\n",
    "In the original Keras [example](https://keras.io/examples/vision/object_detection_using_vision_transformer/) by [Karan V. Dave](https://www.linkedin.com/in/karan-dave-811413164/), an object detection ViT was implemented and trained on the [Caltech 101 dataset](https://data.caltech.edu/records/mzrjq-6wc02) to detect an airplane in the given image.\n",
    "\n",
    "A custom datase was created to this project, that consists of 404 images with 1741 labels / boxes of only one class -- banner. \n",
    "Starging with over 2400 images of different size and format, the pre-processing was done in the previous notebook.\n",
    "The annnotation was done using the open-source tool [makesense.ai](https://www.makesense.ai/) by [Piotr Skalski](https://github.com/SkalskiP) at [Roboflow](https://roboflow.com/).\n",
    "\n",
    "\n",
    "The example requires TensorFlow 2.4 or higher, and [TensorFlow Addons](https://www.tensorflow.org/addons/overview), from which we import the `AdamW` optimizer.\n",
    "TensorFlow Addons can be installed via the following command:\n",
    "\n",
    "```\n",
    "pip install -U tensorflow-addons\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "# import scipy.io # in the original example, this is used for reading Matlab files -- the annotations\n",
    "import shutil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to images and annotations\n",
    "path_images = \"/101_ObjectCategories/airplanes/\"\n",
    "path_annot = \"/Annotations/Airplanes_Side_2/\"\n",
    "\n",
    "path_to_downloaded_file = keras.utils.get_file(\n",
    "    fname=\"caltech_101_zipped\",\n",
    "    origin=\"https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\",\n",
    "    extract=True,\n",
    "    archive_format=\"zip\",  # downloaded file format\n",
    "    cache_dir=\"/\",  # cache and extract in current directory\n",
    ")\n",
    "\n",
    "# Extracting tar files found inside main zip file\n",
    "shutil.unpack_archive(\"/datasets/caltech-101/101_ObjectCategories.tar.gz\", \"/\")\n",
    "shutil.unpack_archive(\"/datasets/caltech-101/Annotations.tar\", \"/\")\n",
    "\n",
    "# list of paths to images and annotations\n",
    "image_paths = [\n",
    "    f for f in os.listdir(path_images) if os.path.isfile(os.path.join(path_images, f))\n",
    "]\n",
    "annot_paths = [\n",
    "    f for f in os.listdir(path_annot) if os.path.isfile(os.path.join(path_annot, f))\n",
    "]\n",
    "\n",
    "image_paths.sort()\n",
    "annot_paths.sort()\n",
    "\n",
    "image_size = 224  # resize input images to this size\n",
    "\n",
    "images, targets = [], []\n",
    "\n",
    "# loop over the annotations and images, preprocess them and store in lists\n",
    "for i in range(0, len(annot_paths)):\n",
    "    # Access bounding box coordinates\n",
    "    annot = scipy.io.loadmat(path_annot + annot_paths[i])[\"box_coord\"][0]\n",
    "\n",
    "    top_left_x, top_left_y = annot[2], annot[0]\n",
    "    bottom_right_x, bottom_right_y = annot[3], annot[1]\n",
    "\n",
    "    image = keras.utils.load_img(\n",
    "        path_images + image_paths[i],\n",
    "    )\n",
    "    (w, h) = image.size[:2]\n",
    "\n",
    "    # resize train set images\n",
    "    if i < int(len(annot_paths) * 0.8):\n",
    "        # resize image if it is for training dataset\n",
    "        image = image.resize((image_size, image_size))\n",
    "\n",
    "    # convert image to array and append to list\n",
    "    images.append(keras.utils.img_to_array(image))\n",
    "\n",
    "    # apply relative scaling to bounding boxes as per given image and append to list\n",
    "    targets.append(\n",
    "        (\n",
    "            float(top_left_x) / w,\n",
    "            float(top_left_y) / h,\n",
    "            float(bottom_right_x) / w,\n",
    "            float(bottom_right_y) / h,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Convert the list to numpy array, split to train and test dataset\n",
    "(x_train), (y_train) = (\n",
    "    np.asarray(images[: int(len(images) * 0.8)]),\n",
    "    np.asarray(targets[: int(len(targets) * 0.8)]),\n",
    ")\n",
    "(x_test), (y_test) = (\n",
    "    np.asarray(images[int(len(images) * 0.8) :]),\n",
    "    np.asarray(targets[int(len(targets) * 0.8) :]),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement multilayer-perceptron (MLP)\n",
    "\n",
    "We use the code from the Keras example\n",
    "[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/)\n",
    "as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the patch creation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    #     Override function to avoid error while saving model\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update(\n",
    "            {\n",
    "                \"input_shape\": input_shape,\n",
    "                \"patch_size\": patch_size,\n",
    "                \"num_patches\": num_patches,\n",
    "                \"projection_dim\": projection_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"transformer_units\": transformer_units,\n",
    "                \"transformer_layers\": transformer_layers,\n",
    "                \"mlp_head_units\": mlp_head_units,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        # return patches\n",
    "        return tf.reshape(patches, [batch_size, -1, patches.shape[-1]])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display patches for an input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 32  # Size of the patches to be extracted from the input images\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(x_train[0].astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "patches = Patches(patch_size)(tf.convert_to_tensor([x_train[0]]))\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"{patches.shape[1]} patches per image \\n{patches.shape[-1]} elements per patch\")\n",
    "\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the patch encoding layer\n",
    "\n",
    "The `PatchEncoder` layer linearly transforms a patch by projecting it into a\n",
    "vector of size `projection_dim`. It also adds a learnable position\n",
    "embedding to the projected vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-378b95ea0053>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mPatchEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_patches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprojection_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_patches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_patches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprojection_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    # Override function to avoid error while saving model\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update(\n",
    "            {\n",
    "                \"input_shape\": input_shape,\n",
    "                \"patch_size\": patch_size,\n",
    "                \"num_patches\": num_patches,\n",
    "                \"projection_dim\": projection_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"transformer_units\": transformer_units,\n",
    "                \"transformer_layers\": transformer_layers,\n",
    "                \"mlp_head_units\": mlp_head_units,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the ViT model\n",
    "\n",
    "The ViT model has multiple Transformer blocks.\n",
    "The `MultiHeadAttention` layer is used for self-attention,\n",
    "applied to the sequence of image patches. The encoded patches (skip connection)\n",
    "and self-attention layer outputs are normalized and fed into a\n",
    "multilayer perceptron (MLP).\n",
    "The model outputs four dimensions representing\n",
    "the bounding box coordinates of an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vit_object_detector(\n",
    "    input_shape,\n",
    "    patch_size,\n",
    "    num_patches,\n",
    "    projection_dim,\n",
    "    num_heads,\n",
    "    transformer_units,\n",
    "    transformer_layers,\n",
    "    mlp_head_units,\n",
    "):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.3)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.3)\n",
    "\n",
    "    bounding_box = layers.Dense(4)(\n",
    "        features\n",
    "    )  # Final four neurons that output bounding box\n",
    "\n",
    "    # return Keras model.\n",
    "    return keras.Model(inputs=inputs, outputs=bounding_box)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment(model, learning_rate, weight_decay, batch_size, num_epochs):\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Compile model.\n",
    "    model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "    checkpoint_filepath = \"logs/\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[\n",
    "            checkpoint_callback,\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "input_shape = (image_size, image_size, 3)  # input image shape\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "# Size of the transformer layers\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]\n",
    "transformer_layers = 4\n",
    "mlp_head_units = [2048, 1024, 512, 64, 32]  # Size of the dense layers\n",
    "\n",
    "\n",
    "history = []\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "vit_object_detector = create_vit_object_detector(\n",
    "    input_shape,\n",
    "    patch_size,\n",
    "    num_patches,\n",
    "    projection_dim,\n",
    "    num_heads,\n",
    "    transformer_units,\n",
    "    transformer_layers,\n",
    "    mlp_head_units,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = run_experiment(\n",
    "    vit_object_detector, learning_rate, weight_decay, batch_size, num_epochs\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "# Saves the model in current path\n",
    "vit_object_detector.save(\"vit_object_detector.h5\", save_format=\"h5\")\n",
    "\n",
    "# To calculate IoU (intersection over union, given two bounding boxes)\n",
    "def bounding_box_intersection_over_union(box_predicted, box_truth):\n",
    "    # get (x, y) coordinates of intersection of bounding boxes\n",
    "    top_x_intersect = max(box_predicted[0], box_truth[0])\n",
    "    top_y_intersect = max(box_predicted[1], box_truth[1])\n",
    "    bottom_x_intersect = min(box_predicted[2], box_truth[2])\n",
    "    bottom_y_intersect = min(box_predicted[3], box_truth[3])\n",
    "\n",
    "    # calculate area of the intersection bb (bounding box)\n",
    "    intersection_area = max(0, bottom_x_intersect - top_x_intersect + 1) * max(\n",
    "        0, bottom_y_intersect - top_y_intersect + 1\n",
    "    )\n",
    "\n",
    "    # calculate area of the prediction bb and ground-truth bb\n",
    "    box_predicted_area = (box_predicted[2] - box_predicted[0] + 1) * (\n",
    "        box_predicted[3] - box_predicted[1] + 1\n",
    "    )\n",
    "    box_truth_area = (box_truth[2] - box_truth[0] + 1) * (\n",
    "        box_truth[3] - box_truth[1] + 1\n",
    "    )\n",
    "\n",
    "    # calculate intersection over union by taking intersection\n",
    "    # area and dividing it by the sum of predicted bb and ground truth\n",
    "    # bb areas subtracted by  the interesection area\n",
    "\n",
    "    # return ioU\n",
    "    return intersection_area / float(\n",
    "        box_predicted_area + box_truth_area - intersection_area\n",
    "    )\n",
    "\n",
    "\n",
    "i, mean_iou = 0, 0\n",
    "\n",
    "# Compare results for 10 images in the test set\n",
    "for input_image in x_test[:10]:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15))\n",
    "    im = input_image\n",
    "\n",
    "    # Display the image\n",
    "    ax1.imshow(im.astype(\"uint8\"))\n",
    "    ax2.imshow(im.astype(\"uint8\"))\n",
    "\n",
    "    input_image = cv2.resize(\n",
    "        input_image, (image_size, image_size), interpolation=cv2.INTER_AREA\n",
    "    )\n",
    "    input_image = np.expand_dims(input_image, axis=0)\n",
    "    preds = vit_object_detector.predict(input_image)[0]\n",
    "\n",
    "    (h, w) = (im).shape[0:2]\n",
    "\n",
    "    top_left_x, top_left_y = int(preds[0] * w), int(preds[1] * h)\n",
    "\n",
    "    bottom_right_x, bottom_right_y = int(preds[2] * w), int(preds[3] * h)\n",
    "\n",
    "    box_predicted = [top_left_x, top_left_y, bottom_right_x, bottom_right_y]\n",
    "    # Create the bounding box\n",
    "    rect = patches.Rectangle(\n",
    "        (top_left_x, top_left_y),\n",
    "        bottom_right_x - top_left_x,\n",
    "        bottom_right_y - top_left_y,\n",
    "        facecolor=\"none\",\n",
    "        edgecolor=\"red\",\n",
    "        linewidth=1,\n",
    "    )\n",
    "    # Add the bounding box to the image\n",
    "    ax1.add_patch(rect)\n",
    "    ax1.set_xlabel(\n",
    "        \"Predicted: \"\n",
    "        + str(top_left_x)\n",
    "        + \", \"\n",
    "        + str(top_left_y)\n",
    "        + \", \"\n",
    "        + str(bottom_right_x)\n",
    "        + \", \"\n",
    "        + str(bottom_right_y)\n",
    "    )\n",
    "\n",
    "    top_left_x, top_left_y = int(y_test[i][0] * w), int(y_test[i][1] * h)\n",
    "\n",
    "    bottom_right_x, bottom_right_y = int(y_test[i][2] * w), int(y_test[i][3] * h)\n",
    "\n",
    "    box_truth = top_left_x, top_left_y, bottom_right_x, bottom_right_y\n",
    "\n",
    "    mean_iou += bounding_box_intersection_over_union(box_predicted, box_truth)\n",
    "    # Create the bounding box\n",
    "    rect = patches.Rectangle(\n",
    "        (top_left_x, top_left_y),\n",
    "        bottom_right_x - top_left_x,\n",
    "        bottom_right_y - top_left_y,\n",
    "        facecolor=\"none\",\n",
    "        edgecolor=\"red\",\n",
    "        linewidth=1,\n",
    "    )\n",
    "    # Add the bounding box to the image\n",
    "    ax2.add_patch(rect)\n",
    "    ax2.set_xlabel(\n",
    "        \"Target: \"\n",
    "        + str(top_left_x)\n",
    "        + \", \"\n",
    "        + str(top_left_y)\n",
    "        + \", \"\n",
    "        + str(bottom_right_x)\n",
    "        + \", \"\n",
    "        + str(bottom_right_y)\n",
    "        + \"\\n\"\n",
    "        + \"IoU\"\n",
    "        + str(bounding_box_intersection_over_union(box_predicted, box_truth))\n",
    "    )\n",
    "    i = i + 1\n",
    "\n",
    "print(\"mean_iou: \" + str(mean_iou / len(x_test[:10])))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates that a pure Transformer can be trained\n",
    "to predict the bounding boxes of an object in a given image,\n",
    "thus extending the use of Transformers to object detection tasks.\n",
    "The model can be improved further by tuning hyper-parameters and pre-training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[makesense.ai](https://github.com/SkalskiP/make-sense)\n",
    "\n",
    "\n",
    "<!--  \n",
    "@MISC{make-sense,\n",
    "   author = {Piotr Skalski},\n",
    "   title = {{Make Sense}},\n",
    "   howpublished = \"\\url{https://github.com/SkalskiP/make-sense/}\",\n",
    "   year = {2019},\n",
    "}\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "631534c0403eaeea2c426c13035f3a869f15d7e4b06ac9a211faa27bb87b2b71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
